{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-bedrock"
      ],
      "metadata": {
        "id": "PnxTSdW_oZ_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-llms-huggingface-api -U -q"
      ],
      "metadata": {
        "id": "CHfMwfh8B4dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.bedrock import Bedrock\n",
        "\n",
        "llm = Bedrock(\n",
        "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    aws_access_key_id=\"Aws access key\",\n",
        "    aws_secret_access_key=\"Aws secret key\",\n",
        "    region_name=\"Region\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "aW0pD1MaqNt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "#ds = load_dataset(\"neural-bridge/rag-dataset-1200\")\n",
        "dataset = load_dataset(path=\"neural-bridge/rag-dataset-1200\", split=\"train\")\n",
        "\n",
        "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
        "for i, context in enumerate(dataset):\n",
        "    with open(Path(\"data\") / f\"context_{i}.txt\", \"w\") as f:\n",
        "        f.write(context[\"context\"])"
      ],
      "metadata": {
        "id": "j14_gePABv9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
        "documents = reader.load_data()\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkC1T-iFBwHJ",
        "outputId": "7c85a694-3190-4f75-ba08-d1f35113a383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5960"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "#embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "XU-C-LH4EJ43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# create the pipeline with transformations\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(),\n",
        "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "        #HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# run the pipeline sync or async\n",
        "nodes = await pipeline.arun(documents=documents[:100])\n",
        "nodes"
      ],
      "metadata": {
        "id": "wLebiCpLCdJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "db = chromadb.PersistentClient(path=\"./neural_bridge\")\n",
        "chroma_collection = db.get_or_create_collection(name=\"neural\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        SentenceSplitter(),\n",
        "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "nodes = await pipeline.arun(documents=documents[:100])\n",
        "len(nodes)"
      ],
      "metadata": {
        "id": "qK-u6ph6ExYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "#from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store, embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "r9_l_ca9ExjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # This is needed to run the query engine\n",
        "#llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=llm,\n",
        "    response_mode=\"tree_summarize\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "rN2ej8nnFduR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Who found the answer to a search query collar george herbert essay?\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5jv6d9ITMnE",
        "outputId": "e4dff461-0912-40fc-ef13-a5fafc1d170f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the context information provided, Francisco Rogers found the answer to the search query \"collar george herbert essay\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2fK6uziExpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}